# AWS Configuration
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_key_here

# SageMaker Configuration
SAGEMAKER_ENDPOINT_NAME=llama3-endpoint
SAGEMAKER_ROLE_ARN=arn:aws:iam::your-account:role/SageMakerExecutionRole

# Vector Database Configuration
VECTOR_DB_TYPE=chroma  # Options: chroma, pinecone, faiss
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=rag-chatbot-index

# Embedding Model Configuration
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# LLM Configuration
LLM_MODEL_NAME=meta-llama/Llama-2-7b-chat-hf
MAX_TOKENS=2048
TEMPERATURE=0.7

# Ollama Configuration (for local inference)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_NAME=llama3.1:8b-instruct
OLLAMA_TIMEOUT=300

# Application Configuration
APP_PORT=8000
APP_HOST=0.0.0.0
DEBUG=True

# Document Processing
UPLOAD_DIR=./documents
PROCESSED_DIR=./processed_docs
