An Optical Character Recognition (OCR) system leveraging Machine Learning

Extracting text from PDF files

PDF (Portable Document Format) is a widely used file format for documents that preserves their original appearance and formatting across different devices and platforms.
 
A native PDF is a PDF of a document that was “born digital” because the PDF was created from an electronic version of a document, rather than from print. 

Native PDFs: When dealing with native PDFs, where text is already digitally encoded, libraries like PyMuPDF or PyPDF2 can efficiently extract text without the need for OCR.

A scanned PDF, on the other hand, is generated by scanning documents and then saving them in PDF format. 

In this section, we’ll look at two approaches of extracting text from PDF files by first converting them to images.

Pre-requisities

Create a virtual environment for Python

```

$ sudo apt install python3 python3-venv
    
$ python3 -m venv .venv
    
$ source .venv/bin/activate

```

Approach 1: pdf2image + pytesseract

The pdf2image library is a Python package that converts PDF documents into PIL Image objects. It leverages popular external tools like Poppler or Ghostscript to perform the conversion.

Python-tesseract is an optical character recognition (OCR) tool for Python. It is a wrapper for Google’s Tesseract-OCR Engine. Tesseract is an open-source OCR Engine that extracts printed or written text from images. It was originally developed by Hewlett-Packard, and development was later taken over by Google.

Now install the Python packages

```

$ pip install pdf2image pytesseract --quiet

```


At first convert the PDF pages to PIL objects and then extract text from these objects using pytesseract’s image_to_string method.

```

import pytesseract
from pdf2image import convert_from_path

# convert to image using resolution 600 dpi 
pages = convert_from_path("scanned.pdf", 600)

# extract text
text_data = ''
for page in pages:
    text = pytesseract.image_to_string(page)
    text_data += text + '\n'
print(text_data)

```

Approach 2: PyMuPDF + EasyOCR

EasyOCR is built on top of deep learning frameworks like PyTorch, which enables it to leverage state-of-the-art algorithms and techniques for OCR.

First, install the required packages:

```
$ pip install pymupdf easyocr --quiet

```

Next, convert the PDF pages into png files using PyMuPDF

```
import fitz

pdffile = "scanned.pdf"
doc = fitz.open(pdffile)
zoom = 4
mat = fitz.Matrix(zoom, zoom)
count = 0
# Count variable is to get the number of pages in the pdf
for p in doc:
    count += 1
for i in range(count):
    val = f"image_{i+1}.png"
    page = doc.load_page(i)
    pix = page.get_pixmap(matrix=mat)
    pix.save(val)
doc.close()

```

OCRmyPDF

OCRmyPDF is an open-source library that uses Tesseract internally and adds an OCR text layer to scanned PDF files, allowing them to be searched or copy-pasted.

```

$ pip install ocrmypdf --quiet

```

AWS Textract

AWS Textract is a fully managed machine learning (ML) service that automatically extracts text from scanned documents provided by Amazon Web Services (AWS).

Scalability: AWS Textract emerges as a powerful choice if the requirement is scalable OCR workflows. 

First, install the boto3 package.

```
$ pip install boto3 --quiet

```

```

import boto3

client = boto3.client('textract')

response = client.detect_document_text(
    Document={
        'S3Object': {
            'Bucket': 'pdf-ocr-files-ploomber',
            'Name': 'scanned.pdf'
        }
    }
)

for item in response["Blocks"]:
    if item["BlockType"] == "LINE":
           print (item["Text"])
           
```

OCR on native PDF files

We’ll be performing text extraction on this PDF using both pytesseract and PyPDF2.

```

$ pip install PyPDF2 --quiet

```

```

import pytesseract
from pdf2image import convert_from_path

pages = convert_from_path("attention.pdf", 600)
text_data = ''
text = pytesseract.image_to_string(pages[1])
text_data += text + '\n'
print(text_data)

```

Next, extract text from the same file using PyPDF2.

```

from PyPDF2 import PdfReader

reader = PdfReader("attention.pdf")
page = reader.pages[1]
print(page.extract_text())


```

Optical Character Recognition (OCR) with machine learning is a technology used to extract text from images, including those embedded within PDF files.

This process converts static, non-editable text in scanned documents or image-based PDFs into machine-readable and editable text.

Image Acquisition: The process begins by acquiring the image data from the PDF. If the PDF is a scanned document, it's essentially an image.

Preprocessing: Before OCR analysis, the image often undergoes preprocessing steps. This can include de-skewing, noise reduction, contrast enhancement, and binarization to improve the clarity of the text for better recognition.

OCR Analysis: The core of the process involves the OCR engine, which uses machine learning algorithms (like neural networks) to analyze the image. These algorithms are trained on vast datasets of text and fonts to recognize individual characters, words, and even the layout of the document.

Character and Word Recognition: The OCR engine identifies patterns in the image that correspond to characters and words. Machine learning models help in accurately distinguishing between similar-looking characters and handling variations in fonts, sizes, and styles.

Text Extraction: Once the characters and words are recognized, the OCR software reconstructs them into a searchable and editable text format. This extracted text can then be copied, pasted, searched, and edited.

Output: The output is typically a new version of the PDF with a hidden text layer, making the document searchable and selectable, or the extracted text can be provided in other formats like plain text, Word documents, or structured data.

Workflow:

The user uploads an image through the frontend.
The frontend sends the image to the FastAPI backend's image upload endpoint.
The FastAPI backend receives the image, potentially saves it, and then sends it to the chosen OCR engine (Tesseract, EasyOCR, or a cloud API).
The OCR engine processes the image and extracts the text.
The extracted text is sent back to the FastAPI backend.
The FastAPI backend returns the extracted text to the frontend.
The frontend displays the extracted text to the user.

What is machine learning OCR?

Machine learning OCR  is a technology that uses machine learning algorithms to recognise and extract text from images or scanned documents. 

While this task is easy for humans, it is very complex for software.  

For software, any image is a set of pixels with different colours, grayscale, and other attributes. 

Machine learning OCR tools have to identify the pixel groups which together form the shape of the English alphabet. 

The task is made even more challenging because of factors like:

Varying font sizes and shapes
Handwritten text with different writing styles
Blurry or low-quality images
Multiple text blocks in various parts of the image

However, machine learning OCR technologies solve the problem using pre-trained models (or algorithms) to scan the image and recognize patterns and features. 

Data scientists train the models using large amounts of labeled data (or images with associated answers).  

The model uses statistical techniques to correlate known pixel groups with text. 

They can then recognize patterns and features in an unknown image and "guess" the text accurately.

How does machine learning OCR work?

A machine learning OCR scans an image and recognizes patterns using pre-trained models, and then “rewrites” the text from what it “read.” 

This process happens in several steps.

1. Data pre-processing
2. Text localisation
3. Text recognition
4. Post processing

1. Data preprocessing

As a first step, most OCR technologies will preprocess the scanned image using techniques like resizing, normalization, and noise reduction to enhance the quality of the input data. 

For instance, the system may:

despeckle or remove any spots
deskew or tilt the scanned document slightly to fix alignment issues
smooth the edges of the text
clean up lines and boxes in the image.
While this step is technically not OCR, it is a critical part of the text extraction process.

2. Text localisation

The next task is to locate the image areas containing text. 

Text regions often have distinct edge information like lines, loops, and contours. 

Additionally, scanned documents may have distinct "objects" or other images mixed in (like a company logo on an invoice document).

Text localization uses techniques like edge detection, object detection, and contour analysis to separate text from other types of images.  

3. Text Recognition

Once the machine learning OCR system has found the text regions, it decomposes that specific image area to identify individual letters and words. 

At this stage, individual characters are called "glyphs." To identify a glyph, the system may match it to a previously stored glyph or look for loops, crosses, and dots to "guess" the letter from its unique patterns.

4. Post processing

Text recognition may have some errors due to variations in fonts, noise, or other factors. 

Post-processing is used to improve the accuracy of results. In this step, the OCR system uses spelling correction and grammar rules to correct the text.

For instance, it may compare the recognised text against a dictionary or employ statistical methods to check the frequency of different words in the text.  

Machine learning + OCR

Traditional machine learning OCR began by preprocessing images, then identifying and recognising the text using rule-based algorithms. 

However, the algorithms faced limitations in the range and volume of document images they could process.

Machine learning OCR evolved into deep learning OCR that uses different types of neural networks to improve the text extraction process. 

Convoluted neural networks identify different image regions and text blocks. 

Recurrent neural networks identify the words and derive meaning from them. Together, they convert scanned document images to analyzable data with the accuracy of a human but at a much faster speed.

What is deep learning OCR?

Deep learning OCR is the next stage of the development of machine learning OCR. 

Deep learning OCR uses a technology called neural networks. 

Neural networks are made of hundreds of thousands of interconnected software nodes that communicate with each other while processing data.

Every node in a neural network solves a small part of the problem before passing the data to the next node. 

The whole network works together to improve OCR accuracy and capability.  

Complex neural networks are deep because they have several hidden layers that process data repeatedly over time. 

The system trains the network on various datasets to learn and extract complex text patterns from all types of images.  

To be more specific, deep learning OCR uses two main types of neural networks to perform different tasks.  

Convoluted Neural Networks (CNN) for computer vision tasks and Recurrent Neural Network (RNN) for NLP tasks.

Convoluted neural networks (CNNs)

CNNs contain convolutional layers that transform the input data before passing it to the subsequent layer. 

The term "convolving" originates from mathematics, where it refers to the process of combining data. Convolving is done using matrices, filters of the mathematical world. 

The calculations involved in convolutions are complex, but the underlying concept is akin to a sliding window examining small patches of the image and extracting relevant information.

Recurrent neural networks (RNNs)

RNNs are neural networks with nodes that have a memory-like component. It allows the nodes to remember past information as they process new inputs.

RNNs analyze text one character at a time, considering the surrounding characters to make predictions or infer missing details. 

RNNs recognise the context of the text, such as capturing dependencies between characters and words.  

For instance, in OCR, RNNs can predict the next character in a word based on the characters processed so far, or they can identify specific words or phrases based on the preceding text.

How does deep learning OCR work?

Deep learning OCR also has preprocessing and post-processing steps like the previous generation of machine learning OCR. 

But in between, instead of traditional ML models, the data is fed into CNN and RNN systems.  

Feature extraction

After preprocessing, the data is fed into CNNs. CNNs are primarily responsible for extracting visual features from images or documents. 

They analyze the input data and capture patterns, edges, textures, and other visual characteristics relevant to OCR.

Once the visual features are extracted, the output from the CNNs is further processed to segment the text into individual characters or words. This step involves identifying boundaries or separating different text regions within the image or document. Accurate segmentation is crucial for enabling proper recognition in subsequent stages.

Contextual analysis

The segmented characters or words are then fed into the RNN part of the OCR system. RNNs, with their sequential memory, analyze the characters or words sequentially, considering the context and dependencies between them. 

This allows the system to understand the text's meaning, capture language patterns, and improve recognition accuracy.  

Integrating visual feature extraction through CNNs and contextual understanding through RNNs enhances the system's ability to handle various fonts, languages, and document layouts, making it suitable for diverse OCR applications.

How to train an OCR model?

Gather labeled images with diverse fonts, sizes, and backgrounds that match your OCR needs. Standardize images (e.g., resize, grayscale) and ensure accurate labeling.

Choose a model architecture like Tesseract, CRNN, or a custom option based on your text’s complexity.

Train the model using the labeled dataset, adjusting hyperparameters and monitoring accuracy. This may require substantial computing power.

Optical Character Recognition (OCR) + BIM (Building Information Modeling) 

Optical Character Recognition (OCR) combined with machine learning can be utilized to extract data from BIM (Building Information Modeling) images, transforming visual information into structured, machine-readable data.

Image Input: BIM images, such as blueprints, floor plans, or detailed component drawings, are provided as input to the OCR system. These images can be in various formats like PDF, JPEG, or PNG.

Text Detection: Machine learning models, often based on deep learning architectures like Convolutional Neural Networks (CNNs), are trained to identify and locate text regions within these images. This involves distinguishing text from other graphical elements and handling variations in font, size, and orientation.

Character Recognition: Once text regions are identified, OCR algorithms convert the detected text into machine-readable characters. This process leverages trained models that recognize individual characters and words, even in challenging conditions like low resolution or distorted text.

Data Extraction and Structuring: After character recognition, further machine learning techniques can be applied to extract specific data points and structure them according to BIM requirements. 

This can involve:

Named Entity Recognition (NER): Identifying and classifying key entities like room names, dimensions, material specifications, or component IDs.

Relationship Extraction: Understanding the relationships between different data points, such as associating a dimension with a specific wall or a material with a particular component.

Template-based Extraction: 

For standardized BIM documents, pre-defined templates can guide the extraction of specific fields.

Layout Analysis: Understanding the spatial arrangement of text and graphics to infer meaning and context within the BIM image.

Data Transformation:

The extracted and structured data can then be transformed into a format compatible with BIM software or databases, such as IFC (Industry Foundation Classes), XML, or JSON. 

This allows for seamless integration into the BIM workflow.

Utilizing LLM models, OCR, and CNNs for image processing 

Integration Approaches

Sequential Processing: OCR extracts text, CNN extracts visual features, and then both are fed to the LLM for a holistic understanding.

Multimodal LLMs: Some LLMs are designed to handle both text and image inputs directly, and allowing for more integrated reasoning.

1. Optical Character Recognition (OCR) for Text Extraction

Purpose: OCR is used to extract text content directly from images. 

This converts visual information into a machine-readable text format.

Implementation: Libraries like pytesseract (a Python wrapper for Tesseract OCR) are commonly used.

2. Convolutional Neural Networks (CNNs) for Image Feature Extraction

Purpose: CNNs are excellent for extracting visual features and patterns from images. 

These features can represent objects, shapes, textures, or even the overall layout.

Implementation: Frameworks like TensorFlow or PyTorch are used to build or utilize pre-trained CNN models (e.g., ResNet, VGG, Inception).

Large Language Models (LLMs) for Interpretation and Analysis

Purpose: LLMs can process and understand the extracted text and potentially incorporate visual features for more comprehensive analysis, summarization, or question answering.

Implementation: Accessing LLMs through APIs (e.g., OpenAI's GPT models, Google's Gemini) or using local LLMs with libraries like transformers and ollama with langchain.

An Optical Character Recognition (OCR) system leveraging Machine Learning with a FastAPI backend and a web frontend involves the following components.

1. Machine Learning (ML) Component

OCR Engine: This is the core of the system, responsible for extracting text from images. Popular choices include:

Tesseract: A widely used open-source OCR engine, often integrated with Python using pytesseract. 

It utilizes neural network-based approaches, including Long Short-Term Memory (LSTM) networks for improved accuracy.

EasyOCR: Another open-source option known for its ease of use and support for multiple languages, often employing deep learning for object recognition.
Cloud-based APIs: Services like Amazon Textract, Azure AI Vision, or Google Cloud Vision provide robust, pre-trained OCR models as APIs, offering high accuracy and scalability.

Preprocessing: Image processing techniques (e.g., binarization, noise reduction, deskewing) may be applied to improve the image quality before feeding it to the OCR engine, enhancing text extraction accuracy.

2. Backend (FastAPI)

API Endpoints: FastAPI is used to create a high-performance API that handles requests from the frontend. 

Key endpoints include:

Image Upload: An endpoint (e.g., app.post("/upload_image")) to receive uploaded images from the frontend. 

This typically involves handling UploadFile objects.

OCR Processing: An endpoint that triggers the OCR process, sending the image to the chosen OCR engine and returning the extracted text. This can be asynchronous (async def) for better responsiveness.

Integration with ML Component: The FastAPI application integrates with the chosen OCR engine or API to perform the text extraction.

Data Handling: FastAPI can manage saving uploaded images, processing the OCR results, and returning the extracted text (e.g., in JSON format).

3. Frontend (Web Interface):

User Interface: A web interface (e.g., built with HTML, CSS, and JavaScript, potentially using frameworks like React or Vue.js) allows users to interact with the OCR system.
Image Upload: Users can upload images through a form or drag-and-drop interface.

API Interaction: JavaScript (e.g., using Fetch API or Axios) sends the uploaded image to the FastAPI backend's image upload endpoint.
Displaying Results: The frontend receives the OCR results from the backend and displays the extracted text to the user, potentially allowing for editing or further actions.

How to run a local model for Text Recognition in images?

Large Language Models (LLMs) are changing how we extract information from documents.

Traditional OCR works like a scanner with copy-paste, it finds text on a page and spits it out.

LLM-based OCR works more like an assistant.

The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.

Very Deep Convolutional Networks for Large-Scale Image Recognition(VGG-16)

The VGG-16 is a popular pre-trained models for image classification.

Vision Transformer (ViT)

Vision Transformer (ViT) is a transformer adapted for computer vision tasks. 

LLaVA

To use LLaVA for image recognition, you will need to set up a local environment with the necessary libraries, encode the image into a base64 string, and then send a prompt to the LLaVA model through an API call. You can use tools like Pillow to open the image and base64 for encoding. 

The LLaVA model then processes both the image and text prompt to generate a response that can include extracted text from the image (OCR) and other visual details. 

For fine-tuning: Open-source models like LLaVA offer flexibility for customization and can be fine-tuned on custom datasets to meet specific needs. 

1. Set up your environment

Install necessary libraries, such as Pillow for image processing, requests for API calls, and base64 for encoding.

You may need to set up a local API server for LLaVA (e.g., using Ollama) to send requests to. 

2. Prepare the image

Open the image using Pillow and convert it to a byte stream.

Encode the image's byte stream into a base64 string. 

This allows the image data to be sent within a text-based API request.

```

from PIL import Image
import base64
from io import BytesIO
with open(image_path, "rb") as image_file: image_data = BytesIO(image_file.read())
base64_image = base64.b64encode(image_data.getvalue()).decode('utf-8')

```

3. Create the prompt

Create a text prompt that clearly instructs the model what to do. 

For OCR, you can specifically ask it to transcribe any text in the image.

You can also ask for other details about the image, such as object detection or a general description.

```
query = "Describe this image and transcribe any text in it." 

```

4. Send the API request

Use the requests library to send a POST request to your LLaVA API endpoint.

Include the base64-encoded image and the text prompt in the request payload. The exact format will depend on the specific API you are using.

```
response = requests.post(url, json={"image": base64_image, "query": query})

```
This will likely involve sending a JSON payload to an endpoint like Ollama at http://localhost:11434/api/generate 

5. Process the response

The model's response will be a text string containing the requested information.

Parse the response to extract the relevant information, such as the transcribed text from the image and the description. 

The response will be in a JSON format, so you will need to parse it accordingly.

```
result_text = response.json()['response']
print(result_text) 

```

OCR with llama3.2-vision

1. Install Ollama

First, head to https://ollama.com/download.

2. Pull the Llama 3.2 Vision Model

Open your command prompt or terminal. 

We'll download the Llama 3.2 Vision model using Ollama.

The models are designed for multimodal tasks, capable of processing both text and images. 

For the smaller model (11B, needs at least 8GB of VRAM)

```

$ ollama pull llama3.2-vision:11b

```

3. Run the Model

Once the model is downloaded, run it locally with:

```

$ ollama run llama3.2-vision

```

4. Install ollama-ocr

To easily process images, we'll use the ollama-ocr Python library. 

```

$ pip install ollama-ocr

```

5. OCR

```

from ollama_ocr import OCRProcessor

ocr = OCRProcessor(model_name='llama3.2-vision:11b')

result = ocr.process_image(
    image_path="./your_image.jpg",
    format_type="text"
)
print(result)

```

Replace "./your_image.jpg" with the actual path to your image file. 

Save the code as a .py file (e.g., ocr_script.py). 

Run the script from your command prompt.

Utilizing a BERT (Large Language Model), OCR (Optical Character Recognition), and CNNs (Convolutional Neural Networks) for image recognition involves a multimodal approach, integrating both visual and textual information from an image.

1. Image Preprocessing and Feature Extraction (CNN & OCR)

CNN for Visual Features

Load the image using libraries like OpenCV or PIL.

Preprocess the image (resizing, normalization) for a pre-trained CNN model (e.g., ResNet, VGG, Inception).

Extract visual features by passing the image through the CNN, taking the output of a specific layer before the final classification layers. This provides a rich representation of the image's visual content.

OCR for Textual Features

Apply an OCR engine (e.g., Tesseract via pytesseract, EasyOCR, Keras-OCR) to the image to extract any detectable text.

This step converts the image's textual components into a string format.

2. Textual Feature Extraction and Enhancement (BERT)

BERT for Text Embeddings

Feed the extracted text from OCR into a pre-trained BERT model.

Obtain contextualized embeddings for the text. BERT's ability to understand word relationships and context can enhance the textual representation.

Consider using techniques like combining BERT's predictions with spell-checkers to improve accuracy if OCR output is noisy.

3. Multimodal Fusion and Classification

Feature Concatenation or Fusion Layer

Combine the visual features (from CNN) and textual features (from BERT) into a single representation. This can be done by concatenating the feature vectors or using a dedicated multimodal fusion layer (e.g., attention mechanisms).

Classification Layer

Feed the fused feature vector into a classification layer (e.g., a fully connected neural network) to perform the desired image recognition task (e.g., object classification, document classification, scene understanding).

Training

If training a custom model, prepare a dataset with images and corresponding labels.

Train the entire multimodal architecture end-to-end, optimizing the CNN, BERT (if fine-tuning), and the fusion/classification layers.

Python Libraries

Image Processing: Pillow, OpenCV
CNNs: TensorFlow, Keras, PyTorch (with pre-trained models)
OCR: pytesseract, EasyOCR, Keras-OCR
BERT: Hugging Face Transformers library

References

Tesseract  https://github.com/tesseract-ocr/tessdoc

EasyOCR https://github.com/JaidedAI/EasyOCR

OCRmyPDF https://github.com/ocrmypdf/OCRmyPDF

MNIST Dataset http://yann.lecun.com/exdb/mnist/

Building and Managing an LLM-based OCR System with MLflow https://mlflow.org/blog/mlflow-prompt-evaluate

Llama 3.2: Revolutionizing edge AI and vision with open, customizable models https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/

Vision Transformer (ViT) https://huggingface.co/docs/transformers/en/model_doc/vit

LLaVA: Large Language and Vision Assistant https://github.com/haotian-liu/LLaVA/tree/main


