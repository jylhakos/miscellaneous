# System Architecture & API Flow Diagram

## Complete System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              USER BROWSER                                 â”‚
â”‚                         http://localhost:3000                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                            [User uploads image]
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FRONTEND SERVER (Port 3000)                       â”‚
â”‚                           serve_frontend.py                               â”‚
â”‚                                                                           â”‚
â”‚  Purpose: Serve static HTML/CSS/JavaScript files                         â”‚
â”‚  Technology: Python http.server.SimpleHTTPRequestHandler                 â”‚
â”‚  Files Served:                                                            â”‚
â”‚    â€¢ frontend/index.html - Web UI                                        â”‚
â”‚    â€¢ Inline CSS - Styling                                                â”‚
â”‚    â€¢ Inline JavaScript - Functionality                                   â”‚
â”‚                                                                           â”‚
â”‚  âŒ Does NOT process images                                              â”‚
â”‚  âŒ Does NOT perform OCR                                                 â”‚
â”‚  âŒ Does NOT call Ollama                                                 â”‚
â”‚  âœ… Only serves static files to browser                                  â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                        [JavaScript makes fetch() call]
                                    â†“
              POST http://localhost:8000/api/ocr/llm
              Content-Type: multipart/form-data
              Body: {file: image.jpg, prompt: "...", model: "..."}
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         BACKEND API (Port 8000)                           â”‚
â”‚                        fastapi_ocr_service.py                             â”‚
â”‚                                                                           â”‚
â”‚  Purpose: REST API for OCR processing                                    â”‚
â”‚  Technology: FastAPI + Uvicorn                                           â”‚
â”‚                                                                           â”‚
â”‚  Endpoints:                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Traditional OCR (No Ollama)                                     â”‚   â”‚
â”‚  â”‚   POST /api/ocr/pdf      â†’ Tesseract/EasyOCR                   â”‚   â”‚
â”‚  â”‚   POST /api/ocr/image    â†’ Tesseract/EasyOCR                   â”‚   â”‚
â”‚  â”‚   POST /api/ocr/bim      â†’ EasyOCR                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ LLM OCR (Uses Ollama) â­                                        â”‚   â”‚
â”‚  â”‚   POST /api/ocr/llm              â†’ Ollama + Llama 3.2         â”‚   â”‚
â”‚  â”‚   POST /v1/chat/completions      â†’ Ollama + Llama 3.2         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                           â”‚
â”‚  âœ… Processes OCR requests                                               â”‚
â”‚  âœ… Forwards LLM requests to Ollama                                      â”‚
â”‚  âœ… Returns JSON responses                                               â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                    [For LLM endpoints only]
                                    â†“
              POST http://localhost:11434/api/generate
              Content-Type: application/json
              Body: {model: "...", prompt: "...", images: ["base64..."]}
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          LLM SERVICE (Port 11434)                         â”‚
â”‚                              Ollama Server                                â”‚
â”‚                                                                           â”‚
â”‚  Purpose: Local LLM inference                                            â”‚
â”‚  Technology: Ollama                                                      â”‚
â”‚  Model: Llama 3.2 Vision (11B or 90B)                                   â”‚
â”‚                                                                           â”‚
â”‚  Processing Steps:                                                        â”‚
â”‚  1. Receive base64-encoded image + prompt                                â”‚
â”‚  2. Load Llama 3.2 Vision model into memory                             â”‚
â”‚  3. Vision Transformer processes image                                   â”‚
â”‚  4. Language model understands prompt                                    â”‚
â”‚  5. Generate natural language response                                   â”‚
â”‚  6. Return JSON with analysis                                            â”‚
â”‚                                                                           â”‚
â”‚  âœ… Runs AI models locally                                               â”‚
â”‚  âœ… No internet required (after model download)                          â”‚
â”‚  âœ… Privacy-focused (data never leaves your machine)                     â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                            [Response flows back]
                                    â†“
              Ollama â†’ FastAPI â†’ Frontend â†’ Browser â†’ User
```

## Request Flow for LLM Image Recognition

```
Step 1: User Interaction
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User opens http://localhost:3000
User drags image.jpg onto upload area
User types prompt: "Extract all text from this document"
User selects model: llama3.2-vision:11b
User clicks "Analyze Image" button

        â†“

Step 2: Frontend JavaScript
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const formData = new FormData();
formData.append('file', imageFile);
formData.append('prompt', 'Extract all text from this document');
formData.append('model', 'llama3.2-vision:11b');

fetch('http://localhost:8000/api/ocr/llm', {
    method: 'POST',
    body: formData
})

        â†“

Step 3: HTTP Request
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
POST http://localhost:8000/api/ocr/llm
Content-Type: multipart/form-data

Parts:
  - file: [binary image data]
  - prompt: "Extract all text from this document"
  - model: "llama3.2-vision:11b"

        â†“

Step 4: FastAPI Receives Request
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.post("/api/ocr/llm")
async def ocr_llm(file, prompt, model):
    # Save uploaded file
    temp_path = save_temp_file(file)
    
    # Call LLM processor
    processor = LLMOCRProcessor(model=model)
    result = processor.process_image(temp_path, prompt)
    
    # Return result
    return {"text": result, "model": model}

        â†“

Step 5: LLM Processor
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class LLMOCRProcessor:
    def process_image(image_path, prompt):
        # Read and encode image
        image_data = base64.b64encode(open(image_path, 'rb').read())
        
        # Prepare request for Ollama
        payload = {
            'model': 'llama3.2-vision:11b',
            'prompt': prompt,
            'images': [image_data],
            'stream': False
        }
        
        # Send to Ollama
        response = requests.post('http://localhost:11434/api/generate', json=payload)
        return response.json()['response']

        â†“

Step 6: HTTP Request to Ollama
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
POST http://localhost:11434/api/generate
Content-Type: application/json

{
  "model": "llama3.2-vision:11b",
  "prompt": "Extract all text from this document",
  "images": ["/9j/4AAQSkZJRgABAQEAYABgAAD..."],
  "stream": false
}

        â†“

Step 7: Ollama Processing
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Decode base64 image
2. Load Llama 3.2 Vision model (if not in memory)
3. Vision Transformer extracts visual features
4. Language model processes prompt
5. Generate response based on image + prompt
6. Return JSON with AI-generated text

        â†“

Step 8: Ollama Response
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{
  "model": "llama3.2-vision:11b",
  "response": "This document contains the following text:\n\nInvoice #12345\nDate: 2024-10-16\nTotal: $199.99\n..."
}

        â†“

Step 9: Response Flow
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ollama â†’ LLMOCRProcessor â†’ FastAPI â†’ Frontend JavaScript

        â†“

Step 10: Display Result
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Browser receives JSON:
{
  "text": "This document contains the following text:\n\nInvoice #12345...",
  "model": "llama3.2-vision:11b"
}

JavaScript updates DOM:
document.getElementById('result').textContent = response.text;

        â†“

User sees AI-generated analysis on screen!
```

## API Endpoint Comparison

### Traditional OCR (No Ollama)

```
Request:
  POST /api/ocr/image
  FormData: {file: image.jpg, method: 'tesseract'}

Processing:
  FastAPI â†’ Tesseract OCR Engine â†’ Text Extraction

Response:
  {"text": "INVOICE\n12345\nTotal: $199.99", "method": "tesseract"}

Speed: âš¡ Very Fast (< 1 second)
Accuracy: ğŸ“Š Good (85-95%)
Context: âŒ None
```

### LLM OCR (Uses Ollama)

```
Request:
  POST /api/ocr/llm
  FormData: {file: image.jpg, prompt: 'Extract invoice details', model: 'llama3.2-vision:11b'}

Processing:
  FastAPI â†’ Ollama â†’ Llama 3.2 Vision â†’ AI Analysis

Response:
  {
    "text": "This is an invoice with the following details:\n- Invoice Number: 12345\n- Date: Oct 16, 2024\n- Total Amount: $199.99\n- Vendor: ACME Corp",
    "model": "llama3.2-vision:11b"
  }

Speed: ğŸ¢ Slower (5-10 seconds for 11B, 30-60s for 90B)
Accuracy: ğŸ¯ Excellent (95-99%)
Context: âœ… Advanced understanding
```

## Why Three Separate Servers?

### Frontend Server (Port 3000)
**Purpose:** Deliver static files to browser  
**Why separate?**
- Simple HTTP server, no complex logic
- Can be replaced with nginx/Apache in production
- Easy to serve on CDN
- No Python dependencies in frontend

### Backend API (Port 8000)
**Purpose:** Process OCR requests, business logic  
**Why separate?**
- FastAPI handles complex request processing
- Can scale independently
- Different deployment strategy than static files
- Needs Python environment

### LLM Service (Port 11434)
**Purpose:** Run AI models  
**Why separate?**
- Resource-intensive (needs GPU/RAM)
- Can be on different machine
- Ollama can serve multiple applications
- Isolates ML workload

## Security Note

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Internet  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Firewall - Only expose what's needed   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend (3000) â”‚     â”‚  FastAPI (8000)  â”‚     â”‚  Ollama (11434)  â”‚
â”‚  âœ… Public       â”‚â”€â”€â”€â”€â†’â”‚  âœ… Public       â”‚â”€â”€â”€â”€â†’â”‚  âŒ Private      â”‚
â”‚  Static files    â”‚     â”‚  API Gateway     â”‚     â”‚  Internal only   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Production Best Practice:
- Frontend: Serve via CDN or nginx
- Backend: Expose via API gateway
- Ollama: Keep private, only accessible by backend
```

## Summary

âœ… **Frontend Server** (serve_frontend.py):
   - Serves HTML/CSS/JS files
   - Port 3000
   - No image processing

âœ… **Backend API** (fastapi_ocr_service.py):
   - Handles OCR requests
   - Port 8000
   - Routes to Ollama for LLM

âœ… **LLM Service** (Ollama):
   - Runs AI models
   - Port 11434
   - Never directly accessed by frontend

âœ… **Data Flow**:
   User â†’ Frontend â†’ FastAPI â†’ Ollama â†’ LLM â†’ Response

âœ… **Ollama Used By**:
   - /api/ocr/llm â­
   - /v1/chat/completions â­
   
âœ… **Ollama NOT Used By**:
   - /api/ocr/pdf
   - /api/ocr/image
   - /api/ocr/bim
